{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree    # Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # Support Vector Machine \n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.feature_extraction.text as sk_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_names=['Name', 'ScreenName', 'UserID', 'FollowersCount', 'FriendsCount', 'Location', 'Description', 'CreatedAt', 'StatusID', 'Language', 'Place', 'RetweetCount', 'FavoriteCount', 'Text']\n",
    "tweets = pd.read_csv('clinton_trump_tweets.txt', sep=\"\\t\",  encoding=\"ISO-8859-1\", header=None, names=column_names)\n",
    "#print initial data\n",
    "tweets = tweets.drop(['ScreenName', 'FollowersCount', 'FriendsCount', 'CreatedAt', 'StatusID','Language', 'Place', 'RetweetCount', 'FavoriteCount'], axis=1)\n",
    "tweets.Location = tweets.Location.astype(str)\n",
    "tweets.Text = tweets.Text.astype(str)\n",
    "\n",
    "ground_truth = pd.read_table('clinton_trump_user_classes.txt', encoding =\"ISO-8859-1\", dtype=int, names=[\"UserID\", \"TrumpOrClinton\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (10 pts): Remove all retweets first. Remove all users that have less than 20 tweets. You may want to keep the entire tweet content, including hashtags/handles. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[~tweets.Text.str.startswith('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# def keepHashMentions(text):\n",
    "#     hashMentions = []\n",
    "#     for word in text:\n",
    "#         hashMentions.extend(word)\n",
    "#     return \" \".join(hashMentions).strip()\n",
    "# ## keeping tweet including hashtags and mentions here ## \n",
    "# tweets['HashMentions'] = tweets.Text.str.findall('(@\\w+)|(#\\w+)').apply(keepHashMentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all users that have less than 20 tweets. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 keep tweets where UID appears 20+ times\n",
    "tweets = tweets.groupby(\"UserID\").filter(lambda x: len(x) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # Generate a list of mention/hashes that have a frequency of 20+ #\n",
    "# top_hash = pd.Series(tweets['HashMentions'].str.cat(sep=' ').split()).value_counts()\n",
    "# top20 = top_hash[top_hash>=20]\n",
    "# top20List = top20.index.tolist()\n",
    "\n",
    "# # Convert list to set so it will have quick lookup\n",
    "# top20Set = set(top20List)\n",
    "\n",
    "\n",
    "# # generate list of mention/hashes that occur 20+ times from our orginal list \n",
    "# def removeUnder20Mentions(hashMentions):\n",
    "#     mentions =  hashMentions.split()\n",
    "#     mentionsOver20 = []\n",
    "#     for mention in mentions:\n",
    "#         if(mention in top20Set):\n",
    "#             mentionsOver20.append(mention)\n",
    "#     return \" \".join(mentionsOver20)\n",
    "# tweets['HashMentions'] = tweets.HashMentions.apply(removeUnder20Mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new df merging hashmentions and UserID  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashMentionsMerged = tweets.groupby('UserID')['Text'].apply(' '.join).reset_index()\n",
    "# tweetsWithNoHashMentions = tweets.drop('Text', axis=1)\n",
    "# tweets = pd.merge(tweetsWithNoHashMentions, hashMentionsMerged, on='UserID')\n",
    "# tweets = tweets.drop_duplicates('UserID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #remove users that have no hashtag/handles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets = tweets[tweets.Text.apply(lambda x:  bool(x and x.strip()))]\n",
    "#print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrameWithHashTagHandlesAndTruths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##NEW\n",
    "tweets['InputData'] =  tweets['Location']+\" \"+tweets['Description'] +\" \"+tweets['Text']\n",
    "aggregateTweetsHashtags = tweets.groupby('UserID')['InputData'].apply(lambda x: x.str.cat(sep=' '))\n",
    "tweetsPrepareSKText = pd.DataFrame({'User_id': aggregateTweetsHashtags.index, 'All_hashtags': aggregateTweetsHashtags.values})\n",
    "\n",
    "vectorizerInput = sk_text.TfidfVectorizer(max_features = 50000,\n",
    "                             #min_df=.001, \n",
    "                             #max_df=.99,\n",
    "                             stop_words = ['UserID','10','100','11','12','14','15','16','17','18','19','2016',\"00\",\"000\",\"000s\",\"001\",\"007\",\"00am\",\"00pm\",\"01\",\"02\"]\n",
    "                            )\n",
    "\n",
    "                             \n",
    "matrix = vectorizerInput.fit_transform(tweetsPrepareSKText.All_hashtags.values)\n",
    "tdidf = matrix.toarray()\n",
    "df_text = pd.DataFrame(matrix.todense(), index=aggregateTweetsHashtags.index, columns=vectorizerInput.get_feature_names())\n",
    "\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,ground_truth, on = 'UserID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectorizerInput.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Remove locations that are null \n",
    "#tweets['LocationCleaned'] = tweets['Location'].apply(lambda x: x.split(',')[0])\n",
    "\n",
    "# tweetsLocations = tweets[tweets['Location'].notnull() & tweets['Description'].notnull() & tweets['HashMentions'].notnull()]\n",
    "\n",
    "# tweetsLocations = tweetsLocations[(tweetsLocations['Location'] != 'nan') & (tweetsLocations['Location'] != ' ') & (tweetsLocations['Location'] != '')]\n",
    "# print(tweets.shape)\n",
    "# tweets = tweetsLocations[tweetsLocations['Location'].isin(tweetsLocations['Location'].value_counts().nlargest(300).index.tolist())]\n",
    "# tweets['Location'] = tweetsLocations['Location'].map({'NYC':'New York City','New York':'New York City', 'NY':'New York City', 'SF':'San Francisco', 'Hollywood':'Los Angeles'}).fillna(tweetsLocations['Location'])\n",
    "\n",
    "# tweets['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# aggregateTweetsHashtags = tweets.groupby('UserID')['HashMentions'].apply(lambda x: x.str.cat(sep=' '))\n",
    "# tweetsPrepareSKText = pd.DataFrame({'User_id': aggregateTweetsHashtags.index, 'All_hashtags': aggregateTweetsHashtags.values})\n",
    "# vectorizer = sk_text.TfidfVectorizer(max_features = 1000,\n",
    "#                              #min_df=100, \n",
    "#                              #max_df=.8,\n",
    "#                              stop_words = ['UserID']\n",
    "#                              )\n",
    "# matrix = vectorizer.fit_transform(tweetsPrepareSKText.All_hashtags.values)\n",
    "# tdidf = matrix.toarray()\n",
    "# df_text = pd.DataFrame(matrix.todense(), index=aggregateTweetsHashtags.index, columns=vectorizer.get_feature_names())\n",
    "# df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Description ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizerDescription = sk_text.TfidfVectorizer(max_features = 500,\n",
    "#                              #min_df=100, \n",
    "#                              #max_df=.8,\n",
    "#                              stop_words = ['UserID','10','100','11','12','14','15','16','17','18','19','2016']\n",
    "#                             )\n",
    "\n",
    "# matrixDescription = vectorizerDescription.fit_transform(tweets.Description.values)\n",
    "# tdidfDescription = matrixDescription.toarray()\n",
    "# df_text_Description = pd.DataFrame(matrixDescription.todense(), index=tweets.UserID, columns=vectorizerDescription.get_feature_names())\n",
    "# df_text_Description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Description ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# vectorizerLocation = sk_text.TfidfVectorizer(#max_features = 1000,\n",
    "#                              #min_df=100, \n",
    "#                              #max_df=.8,\n",
    "#                              stop_words = ['UserID']                \n",
    "#                             )\n",
    "# matrixLocation = vectorizerLocation.fit_transform(tweets.Location.values)\n",
    "# tdidfLocation = matrixDescription.toarray()\n",
    "# df_text_Location = pd.DataFrame(matrixLocation.todense(), index=tweets.UserID, columns=vectorizerLocation.get_feature_names())\n",
    "# df_text_Location.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ground truths column to UserID of tweets Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #mergeGroundTruths = pd.merge(tweets, ground_truth, on = 'UserID')\n",
    "# #ground truth\n",
    "# dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,ground_truth, on = 'UserID')\n",
    "\n",
    "\n",
    "# ##merge Description and Location into out dataframe \n",
    "# dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Description,on = 'UserID')\n",
    "\n",
    "# dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Location,on = 'UserID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression on Location column, rest of models are below ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Use train_test_split() to split data into training and test sets, where 20 percent of the records go to test set. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataFrameWithHashTagHandlesAndTruths.drop(['UserID','TrumpOrClinton'], axis=1)\n",
    "# grabbing one of the location arrays: USA\n",
    "y = dataFrameWithHashTagHandlesAndTruths.TrumpOrClinton.values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 (20 pts): Train Decision Tree, SVM, Logistic Regression, and Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Decision tree boilerplate\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, gamma='auto') \n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs') \n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,1000,1000), solver='adam', max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3 (20 pts): Train k-NN model. In your report describe the features that you used for k-NN. Perform parameter tuning on k-NN model. Apply 5-fold cross validation and use grid search to find the best K value for k-NN model. Set scoring metric to F1 score (F-measure). Use the best K value identified from grid search to train your k-NN model. Plot the F1 score against K value based on the results you achieved from grid search. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = dict(n_neighbors=range(1,21))\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted',n_jobs=2)\n",
    "grid.fit(X_train, y_train)\n",
    "# view the complete results\n",
    "means = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the results\n",
    "plt.plot(range(1,21),means)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('F1 score based on Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train your model using all data and the best known parameters\n",
    "#TODO: use best n_neighbors from grid search\n",
    "knn = KNeighborsClassifier(n_neighbors=1, weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred =  knn.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
