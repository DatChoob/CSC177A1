{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree    # Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # Support Vector Machine \n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_names=['Name', 'ScreenName', 'UserID', 'FollowersCount', 'FriendsCount', 'Location', 'Description', 'CreatedAt', 'StatusID', 'Language', 'Place', 'RetweetCount', 'FavoriteCount', 'Text']\n",
    "tweets = pd.read_csv('clinton_trump_tweets.txt', sep=\"\\t\",  encoding=\"ISO-8859-1\", header=None, names=column_names)\n",
    "#print initial data\n",
    "tweets = tweets.drop(['ScreenName', 'FollowersCount', 'FriendsCount', 'CreatedAt', 'StatusID','Language', 'Place', 'RetweetCount', 'FavoriteCount'], axis=1)\n",
    "tweets.Location = tweets.Location.astype(str)\n",
    "tweets.Text = tweets.Text.astype(str)\n",
    "\n",
    "ground_truth = pd.read_table('clinton_trump_user_classes.txt', encoding =\"ISO-8859-1\", dtype=int, names=[\"UserID\", \"TrumpOrClinton\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (10 pts): Remove all retweets first. Remove all users that have less than 20 tweets. You may want to keep the entire tweet content, including hashtags/handles. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[~tweets.Text.str.startswith('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def keepHashMentions(text):\n",
    "    hashMentions = []\n",
    "    for word in text:\n",
    "        hashMentions.extend(word)\n",
    "    return \" \".join(hashMentions).strip()\n",
    "## keeping tweet including hashtags and mentions here ## \n",
    "tweets['HashMentions'] = tweets.Text.str.findall('(@\\w+)|(#\\w+)').apply(keepHashMentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all users that have less than 20 tweets. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 keep tweets where UID appears 20+ times\n",
    "tweets = tweets.groupby(\"UserID\").filter(lambda x: len(x) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Generate a list of mention/hashes that have a frequency of 20+ #\n",
    "top_hash = pd.Series(tweets['HashMentions'].str.cat(sep=' ').split()).value_counts()\n",
    "top20 = top_hash[top_hash>=20]\n",
    "top20List = top20.index.tolist()\n",
    "\n",
    "# Convert list to set so it will have quick lookup\n",
    "top20Set = set(top20List)\n",
    "\n",
    "\n",
    "# generate list of mention/hashes that occur 20+ times from our orginal list \n",
    "def removeUnder20Mentions(hashMentions):\n",
    "    mentions =  hashMentions.split()\n",
    "    mentionsOver20 = []\n",
    "    for mention in mentions:\n",
    "        if(mention in top20Set):\n",
    "            mentionsOver20.append(mention)\n",
    "    return \" \".join(mentionsOver20)\n",
    "tweets['HashMentions'] = tweets.HashMentions.apply(removeUnder20Mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashMentionsMerged = tweets.groupby('UserID')['HashMentions'].apply(' '.join).reset_index()\n",
    "tweetsWithNoHashMentions = tweets.drop('HashMentions', axis=1)\n",
    "tweets = pd.merge(tweetsWithNoHashMentions, hashMentionsMerged, on='UserID')\n",
    "tweets = tweets.drop_duplicates('UserID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove users that have no hashtag/handles\n",
    "tweets = tweets[tweets.HashMentions.apply(lambda x:  bool(x and x.strip()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove locations that are null \n",
    "tweets['LocationCleaned'] = tweets['Location'].apply(lambda x: x.split(',')[0])\n",
    "tweetsLocations = tweets[tweets['Location'].notnull()]\n",
    "## manual cleanup of bad location data, fix later\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != 'NAN']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != 'nan']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != ' ']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != '']\n",
    "\n",
    "tweets = tweetsLocations[tweetsLocations['LocationCleaned'].isin(tweetsLocations['LocationCleaned'].value_counts().nlargest(150).index.tolist())]\n",
    "\n",
    "# TODO: maybe try this later on with other locations\n",
    "# tweetsLocations['LocationCleaned'] = tweetsLocations.LocationCleaned.map({'NYC':'New York','New York City':'New York','United States':'USA', 'United States of America': 'USA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregateTweetsHashtags = tweets.groupby('UserID')['HashMentions'].apply(lambda x: x.str.cat(sep=' '))\n",
    "tweetsPrepareSKText = pd.DataFrame({'User_id': aggregateTweetsHashtags.index, 'All_hashtags': aggregateTweetsHashtags.values})\n",
    "vectorizer = sk_text.TfidfVectorizer(max_features = 2000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['TrumpOrClinton','userid','UserID','039','0hour1__','100percfedup','1a','1shawnster','2000shp','2a','8216','8217','8220']\n",
    "                             )\n",
    "matrix = vectorizer.fit_transform(tweetsPrepareSKText.All_hashtags.values)\n",
    "tdidf = matrix.toarray()\n",
    "df_text = pd.DataFrame(matrix.todense(), index=aggregateTweetsHashtags.index, columns=vectorizer.get_feature_names())\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerDescription = sk_text.TfidfVectorizer(max_features = 3000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['UserID']                \n",
    "                            )\n",
    "tweets = tweets[tweets['Description'].notnull()]\n",
    "\n",
    "matrixDescription = vectorizerDescription.fit_transform(tweets.Description.values)\n",
    "tdidfDescription = matrixDescription.toarray()\n",
    "df_text_Description = pd.DataFrame(matrixDescription.todense(), index=tweets.UserID, columns=vectorizerDescription.get_feature_names())\n",
    "df_text_Description.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerLocation = sk_text.TfidfVectorizer(#max_features = 1000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['UserID']                \n",
    "                            )\n",
    "tweets = tweets[tweets['LocationCleaned'].notnull()]\n",
    "\n",
    "matrixLocation = vectorizerLocation.fit_transform(tweets.LocationCleaned.values)\n",
    "tdidfLocation = matrixDescription.toarray()\n",
    "df_text_Location = pd.DataFrame(matrixLocation.todense(), index=tweets.UserID, columns=vectorizerLocation.get_feature_names())\n",
    "df_text_Location.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ground truths column to UserID of tweets Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergeGroundTruths = pd.merge(tweets, ground_truth, on = 'UserID')\n",
    "#ground truth\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,ground_truth, on = 'UserID')\n",
    "##include description\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Description,on = 'UserID')\n",
    "\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Location,on = 'UserID')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression on Location column, rest of models are below ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Use train_test_split() to split data into training and test sets, where 20 percent of the records go to test set. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataFrameWithHashTagHandlesAndTruths.drop(['UserID','TrumpOrClinton'], axis=1)\n",
    "# grabbing one of the location arrays: USA\n",
    "y = dataFrameWithHashTagHandlesAndTruths.TrumpOrClinton.values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 (20 pts): Train Decision Tree, SVM, Logistic Regression, and Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree boilerplate\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, gamma='auto') \n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a few optional parameter\n",
    "# solver uses implementation of gradient descent \n",
    "logreg = LogisticRegression(solver='lbfgs') \n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perception Model: 30 30 30 is number of nerurons etc.\n",
    "### Note: each feature does not have to have same number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hidden_layer_sizes: 3 hidden layers,  each has 30 neurons\n",
    "### solver='adam' is a variation of gradient descent \n",
    "### max_iter determines the number of epochs\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,1000,1000), solver='adam', max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3 (20 pts): Train k-NN model. In your report describe the features that you used for k-NN. Perform parameter tuning on k-NN model. Apply 5-fold cross validation and use grid search to find the best K value for k-NN model. Set scoring metric to F1 score (F-measure). Use the best K value identified from grid search to train your k-NN model. Plot the F1 score against K value based on the results you achieved from grid search. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define the parameter values that should be searched\n",
    "#k_range = list(range(10, 12))\n",
    "# uniform: uniform weights. All points in each neighborhood are weighted equally\n",
    "\n",
    "# distance: weight points by the inverse of their distance.# instantiate and fit the grid  \n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_neighbors=[2,3])\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted',n_jobs=2)\n",
    "grid.fit(X, y)\n",
    "# view the complete results\n",
    "#means = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.plot(k_range, means)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('F1 score based on Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train your model using all data and the best known parameters\n",
    "#TODO: use best n_neighbors from grid search\n",
    "knn = KNeighborsClassifier(n_neighbors=12, weights='uniform')\n",
    "knn.fit(X, y)\n",
    "y_pred =  knn.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
