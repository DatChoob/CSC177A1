{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree    # Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # Support Vector Machine \n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.feature_extraction.text as sk_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.9 s, sys: 2.47 s, total: 41.4 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "column_names=['Name', 'ScreenName', 'UserID', 'FollowersCount', 'FriendsCount', 'Location', 'Description', 'CreatedAt', 'StatusID', 'Language', 'Place', 'RetweetCount', 'FavoriteCount', 'Text']\n",
    "tweets = pd.read_csv('clinton_trump_tweets.txt', sep=\"\\t\",  encoding=\"ISO-8859-1\", header=None, names=column_names)\n",
    "#print initial data\n",
    "tweets = tweets.drop(['ScreenName', 'FollowersCount', 'FriendsCount', 'CreatedAt', 'StatusID','Language', 'Place', 'RetweetCount', 'FavoriteCount'], axis=1)\n",
    "tweets.Location = tweets.Location.astype(str)\n",
    "tweets.Text = tweets.Text.astype(str)\n",
    "\n",
    "ground_truth = pd.read_table('clinton_trump_user_classes.txt', encoding =\"ISO-8859-1\", dtype=int, names=[\"UserID\", \"TrumpOrClinton\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (10 pts): Remove all retweets first. Remove all users that have less than 20 tweets. You may want to keep the entire tweet content, including hashtags/handles. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[~tweets.Text.str.startswith('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.9 s, sys: 301 ms, total: 14.2 s\n",
      "Wall time: 14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def keepHashMentions(text):\n",
    "    hashMentions = []\n",
    "    for word in text:\n",
    "        hashMentions.extend(word)\n",
    "    return \" \".join(hashMentions).strip()\n",
    "## keeping tweet including hashtags and mentions here ## \n",
    "tweets['HashMentions'] = tweets.Text.str.findall('(@\\w+)|(#\\w+)').apply(keepHashMentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all users that have less than 20 tweets. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 keep tweets where UID appears 20+ times\n",
    "tweets = tweets.groupby(\"UserID\").filter(lambda x: len(x) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.29 s ± 59.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "# Generate a list of mention/hashes that have a frequency of 20+ #\n",
    "top_hash = pd.Series(tweets['HashMentions'].str.cat(sep=' ').split()).value_counts()\n",
    "top20 = top_hash[top_hash>=20]\n",
    "top20List = top20.index.tolist()\n",
    "\n",
    "# Convert list to set so it will have quick lookup\n",
    "top20Set = set(top20List)\n",
    "\n",
    "\n",
    "# generate list of mention/hashes that occur 20+ times from our orginal list \n",
    "def removeUnder20Mentions(hashMentions):\n",
    "    mentions =  hashMentions.split()\n",
    "    mentionsOver20 = []\n",
    "    for mention in mentions:\n",
    "        if(mention in top20Set):\n",
    "            mentionsOver20.append(mention)\n",
    "    return \" \".join(mentionsOver20)\n",
    "tweets['HashMentions'] = tweets.HashMentions.apply(removeUnder20Mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashMentionsMerged = tweets.groupby('UserID')['HashMentions'].apply(' '.join).reset_index()\n",
    "tweetsWithNoHashMentions = tweets.drop('HashMentions', axis=1)\n",
    "tweets = pd.merge(tweetsWithNoHashMentions, hashMentionsMerged, on='UserID')\n",
    "tweets = tweets.drop_duplicates('UserID')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove users that have no hashtag/handles\n",
    "tweets = tweets[tweets.HashMentions.apply(lambda x:  bool(x and x.strip()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove locations that are null \n",
    "tweets['LocationCleaned'] = tweets['Location'].apply(lambda x: x.split(',')[0])\n",
    "tweetsLocations = tweets[tweets['Location'].notnull()]\n",
    "## manual cleanup of bad location data, fix later\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != 'NAN']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != 'nan']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != ' ']\n",
    "tweetsLocations = tweetsLocations[tweetsLocations['LocationCleaned'] != '']\n",
    "\n",
    "tweets = tweetsLocations[tweetsLocations['LocationCleaned'].isin(tweetsLocations['LocationCleaned'].value_counts().nlargest(150).index.tolist())]\n",
    "\n",
    "# TODO: maybe try this later on with other locations\n",
    "# tweetsLocations['LocationCleaned'] = tweetsLocations.LocationCleaned.map({'NYC':'New York','New York City':'New York','United States':'USA', 'United States of America': 'USA'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregateTweetsHashtags = tweets.groupby('UserID')['HashMentions'].apply(lambda x: x.str.cat(sep=' '))\n",
    "tweetsPrepareSKText = pd.DataFrame({'User_id': aggregateTweetsHashtags.index, 'All_hashtags': aggregateTweetsHashtags.values})\n",
    "vectorizer = sk_text.TfidfVectorizer(max_features = 2000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['TrumpOrClinton','userid','UserID','039','0hour1__','100percfedup','1a','1shawnster','2000shp','2a','8216','8217','8220']\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashmit/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['trumporclinton'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0hour1___</th>\n",
       "      <th>100ktweets4safety</th>\n",
       "      <th>1uvote</th>\n",
       "      <th>20committee</th>\n",
       "      <th>2351onthelist</th>\n",
       "      <th>2alaw</th>\n",
       "      <th>2xshhhh</th>\n",
       "      <th>4evertruther</th>\n",
       "      <th>4la_volpe</th>\n",
       "      <th>4xalerts</th>\n",
       "      <th>...</th>\n",
       "      <th>yoga</th>\n",
       "      <th>youngcons</th>\n",
       "      <th>youngdems4trump</th>\n",
       "      <th>younggun2016</th>\n",
       "      <th>youranonnews</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yuengling_beer</th>\n",
       "      <th>yusufdfi</th>\n",
       "      <th>zekejmiller</th>\n",
       "      <th>zerohedge</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14763</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24263</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164833</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026541</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0hour1___  100ktweets4safety  1uvote  20committee  2351onthelist  \\\n",
       "UserID                                                                      \n",
       "2391           0.0                0.0     0.0          0.0            0.0   \n",
       "14763          0.0                0.0     0.0          0.0            0.0   \n",
       "24263          0.0                0.0     0.0          0.0            0.0   \n",
       "164833         0.0                0.0     0.0          0.0            0.0   \n",
       "1026541        0.0                0.0     0.0          0.0            0.0   \n",
       "\n",
       "         2alaw  2xshhhh  4evertruther  4la_volpe  4xalerts    ...      yoga  \\\n",
       "UserID                                                        ...             \n",
       "2391       0.0      0.0           0.0        0.0       0.0    ...       0.0   \n",
       "14763      0.0      0.0           0.0        0.0       0.0    ...       0.0   \n",
       "24263      0.0      0.0           0.0        0.0       0.0    ...       0.0   \n",
       "164833     0.0      0.0           0.0        0.0       0.0    ...       0.0   \n",
       "1026541    0.0      0.0           0.0        0.0       0.0    ...       0.0   \n",
       "\n",
       "         youngcons  youngdems4trump  younggun2016  youranonnews  youtube  \\\n",
       "UserID                                                                     \n",
       "2391           0.0              0.0           0.0           0.0      0.0   \n",
       "14763          0.0              0.0           0.0           0.0      0.0   \n",
       "24263          0.0              0.0           0.0           0.0      0.0   \n",
       "164833         0.0              0.0           0.0           0.0      0.0   \n",
       "1026541        0.0              0.0           0.0           0.0      0.0   \n",
       "\n",
       "         yuengling_beer  yusufdfi  zekejmiller  zerohedge  \n",
       "UserID                                                     \n",
       "2391                0.0       0.0          0.0        0.0  \n",
       "14763               0.0       0.0          0.0        0.0  \n",
       "24263               0.0       0.0          0.0        0.0  \n",
       "164833              0.0       0.0          0.0        0.0  \n",
       "1026541             0.0       0.0          0.0        0.0  \n",
       "\n",
       "[5 rows x 2000 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = vectorizer.fit_transform(tweetsPrepareSKText.All_hashtags.values)\n",
    "tdidf = matrix.toarray()\n",
    "df_text = pd.DataFrame(matrix.todense(), index=aggregateTweetsHashtags.index, columns=vectorizer.get_feature_names())\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ground truths column to UserID of tweets Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeGroundTruths = pd.merge(tweets, ground_truth, on = 'UserID')\n",
    "#ground truth\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,ground_truth, on = 'UserID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression on Location column, rest of models are below ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputsV = dataFrameWithHashTagHandlesAndTruths.drop(['UserID','TrumpOrClinton'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Use train_test_split() to split data into training and test sets, where 20 percent of the records go to test set. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = inputsV\n",
    "# grabbing one of the location arrays: USA\n",
    "y = dataFrameWithHashTagHandlesAndTruths.TrumpOrClinton.values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 (20 pts): Train Decision Tree, SVM, Logistic Regression, and Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93762607498625\n",
      "0.9373297002724795\n",
      "0.9369390442124098\n",
      "[[503  63]\n",
      " [ 29 873]]\n"
     ]
    }
   ],
   "source": [
    "## Decision tree boilerplate\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37753825479437814\n",
      "0.614441416893733\n",
      "0.46770139918830983\n",
      "[[  0 566]\n",
      " [  0 902]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jashmit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/jashmit/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, gamma='auto')  # train your model here\n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475912801074617\n",
      "0.7438692098092643\n",
      "0.7285046100293432\n",
      "[[275 291]\n",
      " [ 85 817]]\n"
     ]
    }
   ],
   "source": [
    "# set a few optional parameter\n",
    "# solver uses implementation of gradient descent \n",
    "logreg = LogisticRegression(solver='lbfgs') \n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perception Model: 30 30 30 is number of nerurons etc.\n",
    "### Note: each feature does not have to have same number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 100, 100), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### hidden_layer_sizes: 3 hidden layers,  each has 30 neurons\n",
    "### solver='adam' is a variation of gradient descent \n",
    "### max_iter determines the number of epochs\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(100,100,100), solver='adam', max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.63      0.63       566\n",
      "           1       0.77      0.76      0.76       902\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1468\n",
      "   macro avg       0.70      0.70      0.70      1468\n",
      "weighted avg       0.71      0.71      0.71      1468\n",
      "\n",
      "0.7113631547078413\n",
      "0.7111716621253406\n",
      "0.7112660787394985\n",
      "[[355 211]\n",
      " [213 689]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3 (20 pts): Train k-NN model. In your report describe the features that you used for k-NN. Perform parameter tuning on k-NN model. Apply 5-fold cross validation and use grid search to find the best K value for k-NN model. Set scoring metric to F1 score (F-measure). Use the best K value identified from grid search to train your k-NN model. Plot the F1 score against K value based on the results you achieved from grid search. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# define the parameter values that should be searched\n",
    "k_range = list(range(10, 21))\n",
    "# uniform: uniform weights. All points in each neighborhood are weighted equally\n",
    "\n",
    "# distance: weight points by the inverse of their distance.# instantiate and fit the grid  \n",
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='f1_weighted')\n",
    "grid.fit(X, y)\n",
    "# view the complete results\n",
    "means = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "plt.plot(k_range, means)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('F1 score based on Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model using all data and the best known parameters\n",
    "#TODO: use best n_neighbors from grid search\n",
    "knn = KNeighborsClassifier(n_neighbors=13, weights='uniform')\n",
    "knn.fit(X, y)\n",
    "y_pred =  knn.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
