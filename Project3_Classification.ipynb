{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree    # Decision Tree\n",
    "from sklearn.svm import SVC # Support Vector Machine \n",
    "from sklearn import metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_names=['Name', 'ScreenName', 'UserID', 'FollowersCount', 'FriendsCount', 'Location', 'Description', 'CreatedAt', 'StatusID', 'Language', 'Place', 'RetweetCount', 'FavoriteCount', 'Text']\n",
    "tweets = pd.read_csv('clinton_trump_tweets.txt', sep=\"\\t\",  encoding=\"ISO-8859-1\", header=None, names=column_names)\n",
    "#print initial data\n",
    "tweets.drop(['ScreenName', 'FollowersCount', 'FriendsCount', 'CreatedAt', 'StatusID'], axis=1)\n",
    "tweets.Location = tweets.Location.astype(str)\n",
    "tweets.Text = tweets.Text.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (10 pts): Remove all retweets first. Remove all users that have less than 20 tweets. You may want to keep the entire tweet content, including hashtags/handles. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[~tweets.Text.str.startswith('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def keepHashMentions(text):\n",
    "    hashMentions = []\n",
    "    for word in text:\n",
    "        hashMentions.extend(word)\n",
    "    return \" \".join(hashMentions).strip()\n",
    "## keeping tweet including hashtags and mentions here ## \n",
    "tweets['HashMentions'] = tweets.Text.str.findall('(@\\w+)|(#\\w+)').apply(keepHashMentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all users that have less than 20 tweets. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 keep tweets where UID appears 20+ times\n",
    "tweets = tweets.groupby(\"UserID\").filter(lambda x: len(x) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Generate a list of mention/hashes that have a frequency of 20+ #\n",
    "top_hash = pd.Series(tweets['HashMentions'].str.cat(sep=' ').split()).value_counts()\n",
    "top20 = top_hash[top_hash>=20]\n",
    "top20List = top20.index.tolist()\n",
    "\n",
    "# Convert list to set so it will have quick lookup\n",
    "top20Set = set(top20List)\n",
    "\n",
    "\n",
    "# generate list of mention/hashes that occur 20+ times from our orginal list \n",
    "def removeUnder20Mentions(hashMentions):\n",
    "    mentions =  hashMentions.split()\n",
    "    mentionsOver20 = []\n",
    "    for mention in mentions:\n",
    "        if(mention in top20Set):\n",
    "            mentionsOver20.append(mention)\n",
    "    return \" \".join(mentionsOver20)\n",
    "tweets['FrequencyOver20'] = tweets.HashMentions.apply(removeUnder20Mentions)\n",
    "tweets.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ground truths column to UserID of tweets Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = pd.read_table('clinton_trump_user_classes.txt', encoding =\"ISO-8859-1\", dtype=int, names=[\"UserID\", \"TrumpOrClinton\"])\n",
    "#ground_truth.UserID = ground_truth.UserID.astype(int)\n",
    "mergeGroundTruths = pd.merge(tweets, ground_truth, on = 'UserID')\n",
    "#ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mergeGroundTruths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Use train_test_split() to split data into training and test sets, where 20 percent of the records go to test set. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummyDataFrame = pd.DataFrame({'top20List': top20List})\n",
    "# tweets.HashMentions.str.get_dummies(sep=\" \")\n",
    "# pd.get_dummies(dummyDataFrame,prefix=['top20List'], drop_first=True)\n",
    "# dummyDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing models with dummy Iris data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "# enc = OneHotEncoder(handle_unknown='ignore')\\\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "#Upper Case is for input, lower case is for output \n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# print the shapes of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Applying x train and y train here \n",
    "# X contains all records \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 (20 pts): Train Decision Tree, SVM, Logistic Regression, and Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree boilerplate\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X, y)\n",
    "clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1.0, gamma='auto')  # train your model here\n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# compare actual response values (y_test) with predicted response values (y_pred) \n",
    "\n",
    "# you need to set parameter \"weighted\" because this is multi-class classifier\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "### Results for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # uses gradient descent \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# set a few optional parameter\n",
    "# solver uses implementation of gradient descent \\/\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=200, multi_class='auto')      \n",
    "\n",
    "print(cross_val_score(lr, X, y, cv=10, scoring='f1_weighted').mean())\n",
    "\n",
    "# search for an optimal value of K for KNN\n",
    "k_range = list(range(1, 31))\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='f1_weighted')\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)\n",
    "\n",
    "\n",
    "### Plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('F1 Score based on Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3) \n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# you need to set parameter \"weighted\" because this is multi-class classfier\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perception Model: 30 30 30 is number of nerurons etc.\n",
    "### Note: each feature does not have to have same number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hidden_layer_sizes: 3 hidden layers,  each has 30 neurons\n",
    "### solver='adam' is a variation of gradient descent \n",
    "### max_iter determines the number of epochs\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30), solver='adam', max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dont forget to compare model to ground truths\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conusion Matrix\n",
    "# IMPORTANT: first argument is true values, second argument is predicted values\n",
    "confusion = metrics.confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "print(confusion)\n",
    "print(metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3 (20 pts): Train k-NN model. In your report describe the features that you used for k-NN. Perform parameter tuning on k-NN model. Apply 5-fold cross validation and use grid search to find the best K value for k-NN model. Set scoring metric to F1 score (F-measure). Use the best K value identified from grid search to train your k-NN model. Plot the F1 score against K value based on the results you achieved from grid search. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
