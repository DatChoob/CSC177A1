{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import tree    # Decision Tree\n",
    "from sklearn.linear_model import LogisticRegression # LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC # Support Vector Machine \n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import sklearn.feature_extraction.text as sk_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Prep ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "column_names=['Name', 'ScreenName', 'UserID', 'FollowersCount', 'FriendsCount', 'Location', 'Description', 'CreatedAt', 'StatusID', 'Language', 'Place', 'RetweetCount', 'FavoriteCount', 'Text']\n",
    "tweets = pd.read_csv('clinton_trump_tweets.txt', sep=\"\\t\",  encoding=\"ISO-8859-1\", header=None, names=column_names)\n",
    "#print initial data\n",
    "tweets = tweets.drop(['ScreenName', 'FollowersCount', 'FriendsCount', 'CreatedAt', 'StatusID','Language', 'Place', 'RetweetCount', 'FavoriteCount'], axis=1)\n",
    "tweets.Location = tweets.Location.astype(str)\n",
    "tweets.Text = tweets.Text.astype(str)\n",
    "\n",
    "ground_truth = pd.read_table('clinton_trump_user_classes.txt', encoding =\"ISO-8859-1\", dtype=int, names=[\"UserID\", \"TrumpOrClinton\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (10 pts): Remove all retweets first. Remove all users that have less than 20 tweets. You may want to keep the entire tweet content, including hashtags/handles. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[~tweets.Text.str.startswith('RT')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def keepHashMentions(text):\n",
    "    hashMentions = []\n",
    "    for word in text:\n",
    "        hashMentions.extend(word)\n",
    "    return \" \".join(hashMentions).strip()\n",
    "## keeping tweet including hashtags and mentions here ## \n",
    "tweets['HashMentions'] = tweets.Text.str.findall('(@\\w+)|(#\\w+)').apply(keepHashMentions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all users that have less than 20 tweets. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.2 keep tweets where UID appears 20+ times\n",
    "tweets = tweets.groupby(\"UserID\").filter(lambda x: len(x) >= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Generate a list of mention/hashes that have a frequency of 20+ #\n",
    "top_hash = pd.Series(tweets['HashMentions'].str.cat(sep=' ').split()).value_counts()\n",
    "top20 = top_hash[top_hash>=20]\n",
    "top20List = top20.index.tolist()\n",
    "\n",
    "# Convert list to set so it will have quick lookup\n",
    "top20Set = set(top20List)\n",
    "\n",
    "\n",
    "# generate list of mention/hashes that occur 20+ times from our orginal list \n",
    "def removeUnder20Mentions(hashMentions):\n",
    "    mentions =  hashMentions.split()\n",
    "    mentionsOver20 = []\n",
    "    for mention in mentions:\n",
    "        if(mention in top20Set):\n",
    "            mentionsOver20.append(mention)\n",
    "    return \" \".join(mentionsOver20)\n",
    "tweets['HashMentions'] = tweets.HashMentions.apply(removeUnder20Mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new df merging hashmentions and UserID  ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashMentionsMerged = tweets.groupby('UserID')['HashMentions'].apply(' '.join).reset_index()\n",
    "tweetsWithNoHashMentions = tweets.drop('HashMentions', axis=1)\n",
    "tweets = pd.merge(tweetsWithNoHashMentions, hashMentionsMerged, on='UserID')\n",
    "tweets = tweets.drop_duplicates('UserID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #remove users that have no hashtag/handles ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6174, 7)\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets[tweets.HashMentions.apply(lambda x:  bool(x and x.strip()))]\n",
    "print(tweets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6174, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "New York City     534\n",
       "Los Angeles       343\n",
       "Washington        279\n",
       "Chicago           234\n",
       "London            218\n",
       "United States     203\n",
       "USA               171\n",
       "San Francisco     139\n",
       "Boston            128\n",
       "Florida           125\n",
       "California        119\n",
       "Texas             119\n",
       "Seattle           119\n",
       "Brooklyn          116\n",
       "Toronto           113\n",
       "Atlanta           104\n",
       "Houston           100\n",
       "Austin             95\n",
       "Portland           88\n",
       "Philadelphia       80\n",
       "Dallas             71\n",
       "Denver             68\n",
       "Las Vegas          68\n",
       "Michigan           62\n",
       "New Jersey         62\n",
       "Ohio               61\n",
       "San Diego          60\n",
       "Cleveland          57\n",
       "Canada             55\n",
       "Columbus           54\n",
       "                 ... \n",
       "Detroit            28\n",
       "Missouri           28\n",
       "Louisville         28\n",
       "Milwaukee          27\n",
       "Indiana            27\n",
       "Oakland            27\n",
       "Buffalo            27\n",
       "Dublin             27\n",
       "Arlington          26\n",
       "Alabama            26\n",
       "Connecticut        26\n",
       "Manchester         25\n",
       "Worldwide          25\n",
       "Oklahoma           25\n",
       "Global             25\n",
       "Long Island        24\n",
       "Calgary            24\n",
       "United Kingdom     23\n",
       "Nigeria            23\n",
       "Everywhere         23\n",
       "Sacramento         23\n",
       "Nairobi            23\n",
       "DC                 21\n",
       "Birmingham         21\n",
       "Raleigh            21\n",
       "South Carolina     21\n",
       "Columbia           21\n",
       "Tennessee          21\n",
       "Cambridge          21\n",
       "Bay Area           21\n",
       "Name: LocationCleaned, Length: 97, dtype: int64"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove locations that are null \n",
    "tweets['LocationCleaned'] = tweets['Location'].apply(lambda x: x.split(',')[0])\n",
    "\n",
    "tweetsLocations = tweets[tweets['Location'].notnull() & tweets['Description'].notnull() & tweets['HashMentions'].notnull()]\n",
    "\n",
    "tweetsLocations = tweetsLocations[(tweetsLocations['LocationCleaned'] != 'nan') & (tweetsLocations['LocationCleaned'] != ' ') & (tweetsLocations['LocationCleaned'] != '')]\n",
    "print(tweets.shape)\n",
    "\n",
    "tweets = tweetsLocations[tweetsLocations['LocationCleaned'].isin(tweetsLocations['LocationCleaned'].value_counts().nlargest(300).index.tolist())]\n",
    "tweets['LocationCleaned'] = tweetsLocations['LocationCleaned'].map({'NYC':'New York City','New York':'New York City', 'NY':'New York City', 'SF':'San Francisco', 'Hollywood':'Los Angeles'}).fillna(tweetsLocations['LocationCleaned'])\n",
    "\n",
    "tweets['LocationCleaned'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregateTweetsHashtags = tweets.groupby('UserID')['HashMentions'].apply(lambda x: x.str.cat(sep=' '))\n",
    "tweetsPrepareSKText = pd.DataFrame({'User_id': aggregateTweetsHashtags.index, 'All_hashtags': aggregateTweetsHashtags.values})\n",
    "vectorizer = sk_text.TfidfVectorizer(max_features = 1000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['UserID']\n",
    "                             )\n",
    "matrix = vectorizer.fit_transform(tweetsPrepareSKText.All_hashtags.values)\n",
    "tdidf = matrix.toarray()\n",
    "df_text = pd.DataFrame(matrix.todense(), index=aggregateTweetsHashtags.index, columns=vectorizer.get_feature_names())\n",
    "df_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Description ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>...</th>\n",
       "      <th>yes</th>\n",
       "      <th>yet</th>\n",
       "      <th>yoga</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youtuber</th>\n",
       "      <th>yrs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106568768</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447279666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231921777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201499452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17547533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             10  100   11   12   14   15   16   17   18   19 ...   yes  yet  \\\n",
       "UserID                                                       ...              \n",
       "106568768   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "2447279666  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "231921777   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "201499452   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "17547533    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "\n",
       "            yoga  york  you  young  your  youtube  youtuber  yrs  \n",
       "UserID                                                            \n",
       "106568768    0.0   0.0  0.0    0.0   0.0      0.0       0.0  0.0  \n",
       "2447279666   0.0   0.0  0.0    0.0   0.0      0.0       0.0  0.0  \n",
       "231921777    0.0   0.0  0.0    0.0   0.0      0.0       0.0  0.0  \n",
       "201499452    0.0   0.0  0.0    0.0   0.0      0.0       0.0  0.0  \n",
       "17547533     0.0   0.0  0.0    0.0   0.0      0.0       0.0  0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizerDescription = sk_text.TfidfVectorizer(max_features = 1000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['UserID']\n",
    "                            )\n",
    "\n",
    "matrixDescription = vectorizerDescription.fit_transform(tweets.Description.values)\n",
    "tdidfDescription = matrixDescription.toarray()\n",
    "df_text_Description = pd.DataFrame(matrixDescription.todense(), index=tweets.UserID, columns=vectorizerDescription.get_feature_names())\n",
    "df_text_Description.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize Description ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alabama</th>\n",
       "      <th>angeles</th>\n",
       "      <th>antonio</th>\n",
       "      <th>area</th>\n",
       "      <th>arizona</th>\n",
       "      <th>arlington</th>\n",
       "      <th>atlanta</th>\n",
       "      <th>austin</th>\n",
       "      <th>australia</th>\n",
       "      <th>baltimore</th>\n",
       "      <th>...</th>\n",
       "      <th>toronto</th>\n",
       "      <th>uk</th>\n",
       "      <th>united</th>\n",
       "      <th>usa</th>\n",
       "      <th>vancouver</th>\n",
       "      <th>vegas</th>\n",
       "      <th>virginia</th>\n",
       "      <th>washington</th>\n",
       "      <th>worldwide</th>\n",
       "      <th>york</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106568768</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447279666</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231921777</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201499452</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.590197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17547533</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 107 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            alabama  angeles  antonio  area  arizona  arlington  atlanta  \\\n",
       "UserID                                                                     \n",
       "106568768       0.0      0.0      0.0   0.0      0.0        0.0      0.0   \n",
       "2447279666      0.0      0.0      0.0   0.0      0.0        0.0      0.0   \n",
       "231921777       0.0      0.0      0.0   0.0      0.0        0.0      0.0   \n",
       "201499452       0.0      0.0      0.0   0.0      0.0        0.0      0.0   \n",
       "17547533        0.0      0.0      0.0   0.0      0.0        0.0      0.0   \n",
       "\n",
       "            austin  australia  baltimore    ...     toronto   uk  united  usa  \\\n",
       "UserID                                      ...                                 \n",
       "106568768      0.0        0.0        0.0    ...         0.0  0.0     0.0  0.0   \n",
       "2447279666     0.0        0.0        0.0    ...         0.0  0.0     0.0  1.0   \n",
       "231921777      0.0        0.0        0.0    ...         0.0  0.0     0.0  0.0   \n",
       "201499452      0.0        0.0        0.0    ...         0.0  0.0     0.0  0.0   \n",
       "17547533       0.0        0.0        0.0    ...         0.0  0.0     0.0  0.0   \n",
       "\n",
       "            vancouver  vegas  virginia  washington  worldwide      york  \n",
       "UserID                                                                   \n",
       "106568768         0.0    0.0       0.0         0.0        0.0  0.000000  \n",
       "2447279666        0.0    0.0       0.0         0.0        0.0  0.000000  \n",
       "231921777         0.0    0.0       0.0         0.0        0.0  0.000000  \n",
       "201499452         0.0    0.0       0.0         0.0        0.0  0.590197  \n",
       "17547533          0.0    0.0       0.0         0.0        0.0  0.000000  \n",
       "\n",
       "[5 rows x 107 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizerLocation = sk_text.TfidfVectorizer(#max_features = 1000,\n",
    "                             #min_df=100, \n",
    "                             #max_df=.8,\n",
    "                             stop_words = ['UserID']                \n",
    "                            )\n",
    "matrixLocation = vectorizerLocation.fit_transform(tweets.LocationCleaned.values)\n",
    "tdidfLocation = matrixDescription.toarray()\n",
    "df_text_Location = pd.DataFrame(matrixLocation.todense(), index=tweets.UserID, columns=vectorizerLocation.get_feature_names())\n",
    "df_text_Location.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ground truths column to UserID of tweets Dataframe ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergeGroundTruths = pd.merge(tweets, ground_truth, on = 'UserID')\n",
    "#ground truth\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(df_text,ground_truth, on = 'UserID')\n",
    "\n",
    "\n",
    "##merge Description and Location into out dataframe \n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Description,on = 'UserID')\n",
    "\n",
    "dataFrameWithHashTagHandlesAndTruths = pd.merge(dataFrameWithHashTagHandlesAndTruths,df_text_Location,on = 'UserID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Logistic Regression on Location column, rest of models are below ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1: Use train_test_split() to split data into training and test sets, where 20 percent of the records go to test set. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataFrameWithHashTagHandlesAndTruths.drop(['UserID','TrumpOrClinton'], axis=1).head(4000)\n",
    "# grabbing one of the location arrays: USA\n",
    "y = dataFrameWithHashTagHandlesAndTruths.head(4000).TrumpOrClinton.values\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 (20 pts): Train Decision Tree, SVM, Logistic Regression, and Neural Networks #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision tree boilerplate\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47093906250000006\n",
      "0.68625\n",
      "0.5585637509266124\n",
      "[[  0 251]\n",
      " [  0 549]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, gamma='auto') \n",
    "clf.fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7790266625839231\n",
      "0.77875\n",
      "0.7551953343731428\n",
      "[[103 148]\n",
      " [ 29 520]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs') \n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nueral Networks ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train= scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perception Model: 30 30 30 is number of nerurons etc.\n",
    "### Note: each feature does not have to have same number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### hidden_layer_sizes: 3 hidden layers,  each has 30 neurons\n",
    "### solver='adam' is a variation of gradient descent \n",
    "### max_iter determines the number of epochs\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(1000,1000,1000), solver='adam', max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.3 (20 pts): Train k-NN model. In your report describe the features that you used for k-NN. Perform parameter tuning on k-NN model. Apply 5-fold cross validation and use grid search to find the best K value for k-NN model. Set scoring metric to F1 score (F-measure). Use the best K value identified from grid search to train your k-NN model. Plot the F1 score against K value based on the results you achieved from grid search. #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# X_train = X_train[:2220]\n",
    "# y_train = y_train[:2220]\n",
    "# X_test = X_test[:555]\n",
    "# y_test = y_test[:555]\n",
    "\n",
    "\n",
    "param_grid = dict(n_neighbors=range(1,21))\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "grid = GridSearchCV(knn, param_grid, cv=5, scoring='f1_weighted',n_jobs=2)\n",
    "grid.fit(X_train, y_train)\n",
    "# view the complete results\n",
    "means = grid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plot the results\n",
    "plt.plot(range(1,21),means)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('F1 score based on Cross-Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# train your model using all data and the best known parameters\n",
    "#TODO: use best n_neighbors from grid search\n",
    "knn = KNeighborsClassifier(n_neighbors=2, weights='uniform')\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred =  knn.predict(X_test)\n",
    "\n",
    "print(metrics.precision_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.recall_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.f1_score(y_test, y_pred, average= 'weighted'))\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
